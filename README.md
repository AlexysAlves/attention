# CS50 AI Project: Attention

### Project Aims:

The aim of this project is to write an AI to predict a masked word in a text sequence.


### Project Requirements:

Complete the implementation of `get_mask_token_index`, `get_color_for_attention_score`, and `visualize_attentions`.
  - The get_mask_token_index function accepts the ID of the mask token (represented as an int) and the tokenizer-generated inputs, which will be of type transformers.BatchEncoding. It should return the index of the mask token in the input sequence of tokens.
    - The index should be 0-indexed. For example, if the third input ID is the mask token ID, then your function should return 2.
    - If the mask token is not present in the input sequence at all, your function should return None.
    - You may assume that there will not be more than one mask token in the input sequence.
    - You may find it helpful to look at the transformers documentation, in particular at the return value of calling a tokenizer, to see what fields the BatchEncoding will have that you might want to access.
  - The get_color_for_attention_score function should accept an attention score (a value between 0 and 1, inclusive) and output a tuple of three integers representing an RGB triple (one red value, one green value, one blue value) for the color to use for that attention cell in the attention diagram.
    - If the attention score is 0, the color should be fully black (the value (0, 0, 0)). If the attention score is 1, the color should be fully white (the value (255, 255, 255)). For attention scores in between, the color should be a shade of gray that scales linearly with the attention score.
    - For a color to be a shade of gray, the red, blue, and green values should all be equal.
    - The red, green, and blue values must all be integers, but you can choose whether to truncate or round the values. For example, for the attention score 0.25, your function may return either (63, 63, 63) or (64, 64, 64), since 25% of 255 is 63.75.
  - The visualize_attentions function accepts a sequence of tokens (a list of strings) as well as attentions, which contains all of the attention scores generated by the model. For each attention head, the function should generate one attention visualization diagram, as by calling generate_diagram.
    - The value attentions is a tuple of tensors (a “tensor” can be thought of as a multi-dimensional array in this context).
    - To index into the attentions value to get a specific attention head’s values, you can do so as attentions[i][j][k], where i is the index of the attention layer, j is the index of the beam number (always 0 in our case), and k is the index of the attention head in the layer.
    - This function contains an existing implementation that generates only a single attention diagram, for the first attention head in the first attention layer. Your task is to extend this implementation to generate diagrams for all attention heads and layers.
    - The generate_diagram function expects the first two inputs to be the layer number and the head number. These numbers should be 1-indexed. In other words, for the first attention head and attention layer (each of which has index 0), layer_number should be 1 and head_number should be 1 as well.



### Usage:

- Run `pip3 install -r requirements.txt` to install this project’s dependencies.
- Run `python3 mask.py`.
- Write a sentence with a mask token to see the predictions and resulting diagrams.
